{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601606254230",
   "display_name": "Python 3.7.9 64-bit ('mlos_python_environment': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9c165de6800fc7bad446a99e8d502bd2d7d630ae2571fc7c1ffdc546d944aea3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# LevelDB parameter tuning using MLOS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## What is Level DB\n",
    "\n",
    "LevelDB is a key value store built using Log structured merge trees (LSMs) [Wiki] (https://en.wikipedia.org/wiki/Log-structured_merge-tree). LevelDB mainly supports the read, write, delete and range query (sorted iteration) operations. \n",
    "\n",
    "Typical to any database system, levelDB also comes with a bunch of parameters which can be tuned according to the workload to get the best performance. Before going to the parameters, we'll briefly describe the working of levelDB. The source code, the architecture and a simple example of how to use levelDB can be found [here](https://github.com/google/leveldb)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## LevelDB working\n",
    "<p align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=images/leveldb-architecture.png>\n",
    "  <img width=\"460\" height=\"100\" src=images/memtablesstable.png>\n",
    "</p>\n",
    "\n",
    "As shown the diagram above, the main components of LevelDB are the MemTable,the SSTable files and the log file. LeveDB is primarily optimized for writes. \n",
    "MemTable is an in memory data structure to which incoming writes are added after they are appended to the log file. MemTables are typically implemented using skip lists or B+ trees. The parameter write_buffer_size (paramter input at DB initialization) can be used to control the size of the MemTable and the log file. \n",
    "\n",
    "Once the MemTable reaches the write_buffer_size (Default 4MB), a new MemTable and log file are created and the original MemTable is made immutable. This immutable MemTable is converted to a new SSTable in the background to be added to the Level 0 of the LSM tree. \n",
    "\n",
    "SSTable: It is a file in which the key value pairs are stored sorted by keys. The size of SSTable is controlled by the parameter called max_file_size (Default 2MB).\n",
    "\n",
    "Once the number of SSTable at Level 0 reaches a certain threshold controlled by the paramter kL0_CompactionTrigger (Default 4), these files are merged with higher level overlapping files. If no files are present in the higher level, the files are combined using merge sort techniques and added to higher level. A new file is created for every 2 MB of data by default. \n",
    "\n",
    "For higher levels from 1 to the maximum number of levels, compaction process (merging process) is triggered when the level gets filled. \n",
    "\n",
    "A detailed explanation of the working of LeveDB is presented [here](https://github.com/google/leveldb/blob/master/doc/impl.md).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## LevelDB paramter tuning using MLOS\n",
    "\n",
    "In this lab we will be tuning some of the important paramters of LevelDB and observe how it affects the performance. The parameters that we will be tuning are write_buffer_size and max_file_size to try to optimize the throughput and latency of LevelDB for Sequential and random workloads. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## LevelDB installation: Instruction on Ubuntu 18.04\n",
    "\n",
    "Follow the commands below to get, compile and install LevelDB\n",
    "\n",
    "- sudo apt update\n",
    "- sudo apt-get install cmake\n",
    "- git clone --recurse-submodules https://github.com/google/leveldb.git\n",
    "- cd leveldb\n",
    "- mkdir -p build && cd build\n",
    "- cmake -DCMAKE_BUILD_TYPE=Release .. && cmake --build .\n",
    "\n",
    "Now, from the ~/leveldb/build directory, you should be able to execute ./db_bench, the microbenchmark which can be used to measure the performance of LevelDB for different workloads. \n",
    "\n",
    "Please take a look at the db_bench.cc file in the ~/leveldb/benchmarks directory and get an idea about the input parameters and workloads that are possible. \n",
    "\n",
    "An example command to run a workload that does random writes of 1M values with value size 100 B is:\n",
    "./db_bench --benchmarks=fillrandom --val_size=100 --num=1000000"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required classes and tools\n",
    "import grpc\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from mlos.Grpc.BayesianOptimizerFactory import BayesianOptimizerFactory\n",
    "from mlos.Logger import create_logger\n",
    "\n",
    "from mlos.Examples.SmartCache import HitRateMonitor, SmartCache, SmartCacheWorkloadGenerator, SmartCacheWorkloadLauncher\n",
    "from mlos.Mlos.SDK import MlosExperiment\n",
    "from mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective\n",
    "from mlos.Spaces import Point, SimpleHypergrid, ContinuousDimension, DiscreteDimension\n",
    "\n",
    "# The optimizer will be in a remote process via grpc, we pick the port here:\n",
    "grpc_port = 50051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "optimizer_microservice = subprocess.Popen(f\"start_optimizer_microservice launch --port {grpc_port}\", shell=True)\n",
    "logger = create_logger('Optimizing Smart Cache', logging_level=logging.WARN)\n",
    "optimizer_service_grpc_channel = grpc.insecure_channel(f'localhost:{grpc_port}')\n",
    "bayesian_optimizer_factory = BayesianOptimizerFactory(grpc_channel=optimizer_service_grpc_channel, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_search_space = SimpleHypergrid(\n",
    "        name='write_buffer_size_config',\n",
    "        dimensions=[\n",
    "            DiscreteDimension('write_buffer_size', min=64*1024, max=32*1024*1024)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Problem\n",
    "#\n",
    "optimization_problem = OptimizationProblem(\n",
    "    parameter_space=parameter_search_space,\n",
    "    objective_space=SimpleHypergrid(name=\"objectives\", dimensions=[ContinuousDimension(name=\"throughput\", min=0, max=1000)]),\n",
    "    objectives=[Objective(name=\"throughput\", minimize=False)]\n",
    ")\n",
    "# create an optimizer proxy that connects to the remote optimizer via grpc:\n",
    "optimizer = bayesian_optimizer_factory.create_remote_optimizer(optimization_problem=optimization_problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block can be removed\n",
    "from mlos.Mlos.SDK import mlos_globals, MlosAgent\n",
    "mlos_globals.init_mlos_global_context()\n",
    "mlos_agent = MlosAgent(\n",
    "    logger=logger,\n",
    "    communication_channel=mlos_globals.mlos_global_context.communication_channel,\n",
    "    shared_config=mlos_globals.mlos_global_context.shared_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9.023958206176758 MB 9462306 15.5 MB/s\n21.169506072998047 MB 22197836 36.7 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 9462306}, {&quot;throughput&quot;: 15.5})\n10.208983421325684 MB 10704895 24.8 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 22197836}, {&quot;throughput&quot;: 36.7})\n13.992161750793457 MB 14671845 35.6 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 22197836}, {&quot;throughput&quot;: 36.7})\n16.306455612182617 MB 17098558 36.2 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 22197836}, {&quot;throughput&quot;: 36.7})\n7.275952339172363 MB 7629389 16.0 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 22197836}, {&quot;throughput&quot;: 36.7})\n30.3869047164917 MB 31862979 36.9 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 22197836}, {&quot;throughput&quot;: 36.7})\n15.966145515441895 MB 16741717 36.8 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 31862979}, {&quot;throughput&quot;: 36.9})\n10.518720626831055 MB 11029678 25.4 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 31862979}, {&quot;throughput&quot;: 36.9})\n23.43724536895752 MB 24575733 36.8 MB/s\noptimal:  ({&quot;write_buffer_size&quot;: 31862979}, {&quot;throughput&quot;: 36.9})\n"
    }
   ],
   "source": [
    "# Please change the leveldb_path to the build directory of your leveldb installation\n",
    "leveldb_path = \"/users/nithinv/leveldb/build/\"\n",
    "command = \"db_bench --benchmarks=fillrandom\"\n",
    "\n",
    "import subprocess\n",
    "def run_workload(write_buffer_size):\n",
    "    result = subprocess.check_output(leveldb_path + command + \" --write_buffer_size=\" + write_buffer_size, shell=True)\n",
    "    stats = (str(result).split(\":\")[-1]).split(\";\")\n",
    "    latency, throughput = float(stats[0].strip().split(\" \")[0]), float(stats[1].strip().split(\" \")[0])\n",
    "    return latency, throughput\n",
    "\n",
    "def initialize_optimizer():\n",
    "    pass\n",
    "\n",
    "optimizer = bayesian_optimizer_factory.create_remote_optimizer(optimization_problem=optimization_problem)\n",
    "def run_optimizer():\n",
    "    # Parameter 1: write_buffer_size: min_value = 64 KB, max_value = 1 GB\n",
    "    # Parameter 2: max_file_size: min_value = 1 MB, max_value = 1 GB\n",
    "    # Optimization parameters: latency and throughput, both returned by run_workload\n",
    "    for i in range(10):\n",
    "        new_config_values = optimizer.suggest()\n",
    "        write_buffer_size = new_config_values[\"write_buffer_size\"]\n",
    "        throughput = run_workload(str(write_buffer_size))[1]\n",
    "        print(str(write_buffer_size / (1024*1024)) + \" MB\", write_buffer_size,  str(throughput) + \" MB/s\")\n",
    "        if i > 0:\n",
    "            print(\"optimal: \", optimizer.optimum())\n",
    "        objectives_df = pd.DataFrame({'throughput': [throughput]})\n",
    "        features_df = new_config_values.to_dataframe()\n",
    "        optimizer.register(features_df, objectives_df)\n",
    "\n",
    "run_optimizer()"
   ]
  },
  {
   "source": [
    "### Reference\n",
    "https://wiesen.github.io/post/leveldb-storage-memtable/\n",
    "\n",
    "https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}